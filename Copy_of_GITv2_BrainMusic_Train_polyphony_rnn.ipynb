{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GITv2_BrainMusic_Train_polyphony_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R-JSJ5d8xkXJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bogula/TestNews/blob/master/Copy_of_GITv2_BrainMusic_Train_polyphony_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4ZJ3q5hftP"
      },
      "source": [
        "Licensed under the Apache [License](https://www.apache.org/licenses)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8lGTfVviZ_R"
      },
      "source": [
        "# Brain Music. Polyphony RNN Training\n",
        "Raquel Bujalance and Cecil Fernandez Briche\n",
        "\n",
        "This colab notebook lets you to train Polyphony RNN for music generation, based on [Magenta library](https://github.com/tensorflow/magenta/tree/master/magenta/models/polyphony_rnn).This model applies language modeling using an LSTM to polyphonic music generation.\n",
        "\n",
        "The notebook has been created to help anyone who wants to try training their own polyphony model using Magenta library, step by step, in colab. We have trained it with different samples linked to emotions, you can see the results in [post].\n",
        "\n",
        "\n",
        "Instructions for running:\n",
        "Make sure to use a GPU runtime, click: Runtime >> Change Runtime Type >> GPU\n",
        "Double-click any of the hidden cells to view the code.\n",
        "\n",
        "Note: If you are going to train a heavy model keep in mind that google can restrict the use of [GPUs](https://research.google.com/colaboratory/faq.html#gpu-availability), \"It is possible that a user who uses Colaboratory for long term calculations has a temporary restriction on the type of hardware available to him or the time during which he can use it. We encourage users with significant computing needs to use the Colaboratory UI with a local execution environment.\"\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQgJ6x2UtPCs"
      },
      "source": [
        "# Environment Setup\n",
        "Install magenta and fluidsynth, a sequence synthesis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uq7cNslgQQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "1aefca51-bf0c-40bf-e785-a7d2f69cfbeb"
      },
      "source": [
        "#@title Install\n",
        "\n",
        "#@markdown Install magenta and fluidsynth as a synthesizer to listen de audios.\n",
        " #@markdown Magenta is compatible with both Python 2 and 3.\n",
        " #@markdown This take some time, specially for fluidsynth installation\n",
        "\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -qU pyfluidsynth pretty_midi\n",
        "\n",
        "!pip install magenta\n",
        "\n",
        "# Hack to allow python to pick up the newly-installed fluidsynth lib. \n",
        "# This is only needed for the hosted Colab environment.\n",
        "import ctypes.util\n",
        "orig_ctypes_util_find_library = ctypes.util.find_library\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return orig_ctypes_util_find_library(lib)\n",
        "ctypes.util.find_library = proxy_find_library"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../fluid-soundfont-gm_3.1-5.1_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.1) ...\n",
            "Preparing to unpack .../libasound2-dev_1.1.3-5ubuntu0.6_amd64.deb ...\n",
            "Unpacking libasound2-dev:amd64 (1.1.3-5ubuntu0.6) over (1.1.3-5ubuntu0.5) ...\n",
            "Preparing to unpack .../libasound2_1.1.3-5ubuntu0.6_amd64.deb ...\n",
            "Unpacking libasound2:amd64 (1.1.3-5ubuntu0.6) over (1.1.3-5ubuntu0.5) ...\n",
            "Preparing to unpack .../libasound2-data_1.1.3-5ubuntu0.6_all.deb ...\n",
            "Unpacking libasound2-data (1.1.3-5ubuntu0.6) over (1.1.3-5ubuntu0.5) ...\n",
            "Selecting previously unselected package libfluidsynth1:amd64.\n",
            "Preparing to unpack .../libfluidsynth1_1.1.9-1_amd64.deb ...\n",
            "Unpacking libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up libasound2-data (1.1.3-5ubuntu0.6) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.1) ...\n",
            "Setting up libasound2:amd64 (1.1.3-5ubuntu0.6) ...\n",
            "Setting up libfluidsynth1:amd64 (1.1.9-1) ...\n",
            "Setting up libasound2-dev:amd64 (1.1.3-5ubuntu0.6) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 8.4 MB/s \n",
            "\u001b[?25h  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting magenta\n",
            "  Downloading magenta-2.1.3-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.3 MB/s \n",
            "\u001b[?25hCollecting sox>=1.3.7\n",
            "  Downloading sox-1.4.1-py2.py3-none-any.whl (39 kB)\n",
            "Collecting pygtrie>=2.3\n",
            "  Downloading pygtrie-2.4.2.tar.gz (35 kB)\n",
            "Requirement already satisfied: tensorflow-probability in /usr/local/lib/python3.7/dist-packages (from magenta) (0.13.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from magenta) (1.15.0)\n",
            "Requirement already satisfied: pretty-midi>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from magenta) (0.2.9)\n",
            "Requirement already satisfied: dopamine-rl<=3.0.1 in /usr/local/lib/python3.7/dist-packages (from magenta) (1.0.5)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from magenta) (0.16.2)\n",
            "Collecting numba<0.50\n",
            "  Downloading numba-0.49.1-cp37-cp37m-manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 42.4 MB/s \n",
            "\u001b[?25hCollecting python-rtmidi<1.2,>=1.1\n",
            "  Downloading python-rtmidi-1.1.2.tar.gz (204 kB)\n",
            "\u001b[K     |████████████████████████████████| 204 kB 63.7 MB/s \n",
            "\u001b[?25hCollecting mir-eval>=0.4\n",
            "  Downloading mir_eval-0.6.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting mido==1.2.6\n",
            "  Downloading mido-1.2.6-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting librosa<0.8.0,>=0.6.2\n",
            "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 38.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from magenta) (2.4.1)\n",
            "Collecting tensor2tensor\n",
            "  Downloading tensor2tensor-1.15.7-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 46.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from magenta) (1.4.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from magenta) (2.5.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from magenta) (0.12.0)\n",
            "Collecting note-seq\n",
            "  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\n",
            "\u001b[K     |████████████████████████████████| 210 kB 35.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from magenta) (7.1.2)\n",
            "Requirement already satisfied: matplotlib>=1.5.3 in /usr/local/lib/python3.7/dist-packages (from magenta) (3.2.2)\n",
            "Collecting sk-video\n",
            "  Downloading sk_video-1.1.10-py2.py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 25.0 MB/s \n",
            "\u001b[?25hCollecting dm-sonnet\n",
            "  Downloading dm_sonnet-2.0.0-py3-none-any.whl (254 kB)\n",
            "\u001b[K     |████████████████████████████████| 254 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from magenta) (0.36.2)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from magenta) (4.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from magenta) (1.19.5)\n",
            "Requirement already satisfied: gin-config>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl<=3.0.1->magenta) (0.4.0)\n",
            "Requirement already satisfied: opencv-python>=3.4.1.15 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl<=3.0.1->magenta) (4.1.2.30)\n",
            "Requirement already satisfied: gym>=0.10.5 in /usr/local/lib/python3.7/dist-packages (from dopamine-rl<=3.0.1->magenta) (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->dopamine-rl<=3.0.1->magenta) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.10.5->dopamine-rl<=3.0.1->magenta) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta) (1.0.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa<0.8.0,>=0.6.2->magenta) (0.10.3.post1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.3->magenta) (2.8.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir-eval>=0.4->magenta) (0.16.0)\n",
            "Collecting llvmlite<=0.33.0.dev0,>=0.31.0.dev0\n",
            "  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<0.50->magenta) (57.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa<0.8.0,>=0.6.2->magenta) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa<0.8.0,>=0.6.2->magenta) (2.20)\n",
            "Requirement already satisfied: dm-tree>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta) (0.1.6)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta) (1.12.1)\n",
            "Requirement already satisfied: tabulate>=0.7.5 in /usr/local/lib/python3.7/dist-packages (from dm-sonnet->magenta) (0.8.9)\n",
            "Requirement already satisfied: bokeh>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta) (2.3.3)\n",
            "Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta) (2.1.0)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta) (1.1.5)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta) (5.5.0)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta) (3.17.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from note-seq->magenta) (21.2.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta) (5.1.1)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta) (21.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note-seq->magenta) (3.13)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from intervaltree>=2.1.0->note-seq->magenta) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=0.12.0->note-seq->magenta) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.1->note-seq->magenta) (2018.9)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->note-seq->magenta) (5.0.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->note-seq->magenta) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->IPython->note-seq->magenta) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->note-seq->magenta) (0.7.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->magenta) (2.5.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->magenta) (1.1.1)\n",
            "Collecting mesh-tensorflow\n",
            "  Downloading mesh_tensorflow-0.1.19-py3-none-any.whl (366 kB)\n",
            "\u001b[K     |████████████████████████████████| 366 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting kfac\n",
            "  Downloading kfac-0.2.3-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 58.7 MB/s \n",
            "\u001b[?25hCollecting gevent\n",
            "  Downloading gevent-21.8.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 66.4 MB/s \n",
            "\u001b[?25hCollecting gunicorn\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta) (1.7.1)\n",
            "Collecting tensorflow-gan\n",
            "  Downloading tensorflow_gan-2.0.0-py2.py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 56.3 MB/s \n",
            "\u001b[?25hCollecting pypng\n",
            "  Downloading pypng-0.0.20.tar.gz (649 kB)\n",
            "\u001b[K     |████████████████████████████████| 649 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.13.0-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 58.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta) (4.41.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta) (1.12.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta) (3.1.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta) (1.1.4)\n",
            "Collecting bz2file\n",
            "  Downloading bz2file-0.98.tar.gz (11 kB)\n",
            "Collecting tensorflow-probability\n",
            "  Downloading tensorflow_probability-0.7.0-py2.py3-none-any.whl (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 55.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta) (2.23.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tensor2tensor->magenta) (4.1.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->tensor2tensor->magenta) (7.1.2)\n",
            "Collecting zope.event\n",
            "  Downloading zope.event-4.5.0-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp37-cp37m-manylinux2010_x86_64.whl (251 kB)\n",
            "\u001b[K     |████████████████████████████████| 251 kB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: greenlet<2.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from gevent->tensor2tensor->magenta) (1.1.0)\n",
            "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta) (1.32.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta) (3.0.1)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta) (1.26.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client->tensor2tensor->magenta) (0.17.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client->tensor2tensor->magenta) (1.53.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor->magenta) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor->magenta) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client->tensor2tensor->magenta) (4.7.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client->tensor2tensor->magenta) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tensor2tensor->magenta) (2.10)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->tensor2tensor->magenta) (1.5.2)\n",
            "Collecting kfac\n",
            "  Downloading kfac-0.2.2-py2.py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 65.3 MB/s \n",
            "\u001b[?25h  Downloading kfac-0.2.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->tensor2tensor->magenta) (1.2.1)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (0.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (1.1.2)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (1.34.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (2.5.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (2.5.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->magenta) (3.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->magenta) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->magenta) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->magenta) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->magenta) (3.3.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->magenta) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->magenta) (4.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->magenta) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow->magenta) (3.5.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tensor2tensor->magenta) (2.7.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta) (0.3.4)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta) (1.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta) (2.3)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->magenta) (5.2.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan->tensor2tensor->magenta) (0.12.0)\n",
            "Building wheels for collected packages: librosa, mir-eval, pygtrie, python-rtmidi, bz2file, pypng\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612900 sha256=962132cff311fd8c5420381a2a47ea6b88debfbd9bb471e39f263b5db661a151\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/9e/42/3224f85730f92fa2925f0b4fb6ef7f9c5431a64dfc77b95b39\n",
            "  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-eval: filename=mir_eval-0.6-py3-none-any.whl size=96514 sha256=75e7bd2f6c8bbb7448040040e0a96f88b4a0d4f1af7fc347d71f7993347eba71\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/28/2d/006dbad29550bac8daf049ff34fa882655a7d3e77f3b67595e\n",
            "  Building wheel for pygtrie (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pygtrie: filename=pygtrie-2.4.2-py3-none-any.whl size=19061 sha256=130e962e6844126094dbcbd1b85d928410fbe78fcbd368c32e3a05f4d235b471\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/f8/ba/1d828b1603ea422686eb694253a43cb3a5901ea4696c1e0603\n",
            "  Building wheel for python-rtmidi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-rtmidi: filename=python_rtmidi-1.1.2-cp37-cp37m-linux_x86_64.whl size=389809 sha256=fc752d8063bdc4b953740cbb1477519a4e1f2b7b1aa603eb34c7820a394bd63f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/0c/e1/ccca9af1590f715067166c3199f6b639a26a152f61d4e79397\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bz2file: filename=bz2file-0.98-py3-none-any.whl size=6883 sha256=197184bb251228bdfb2a99014cb3d1be1c0e66f34d2e0a8bd51291e6c5fbb0f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/ce/8d/b5f76b602b16a8a39f2ded74189cf5f09fc4a87bea16c54a8b\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypng: filename=pypng-0.0.20-py3-none-any.whl size=67177 sha256=f7f2c20f30eed676c48b493a7c50571c76b0f61cb7575d708e006b28d4e955cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/64/43/dfd10cf95dc1687dc5350e861321ecd9a5d76b7c3d96fa1dc6\n",
            "Successfully built librosa mir-eval pygtrie python-rtmidi bz2file pypng\n",
            "Installing collected packages: llvmlite, numba, zope.interface, zope.event, tensorflow-probability, mido, tf-slim, tensorflow-gan, tensorflow-addons, pypng, pydub, mesh-tensorflow, librosa, kfac, gunicorn, gevent, bz2file, tensor2tensor, sox, sk-video, python-rtmidi, pygtrie, note-seq, mir-eval, dm-sonnet, magenta\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.13.0\n",
            "    Uninstalling tensorflow-probability-0.13.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.13.0\n",
            "  Attempting uninstall: mido\n",
            "    Found existing installation: mido 1.2.10\n",
            "    Uninstalling mido-1.2.10:\n",
            "      Successfully uninstalled mido-1.2.10\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.8.1\n",
            "    Uninstalling librosa-0.8.1:\n",
            "      Successfully uninstalled librosa-0.8.1\n",
            "Successfully installed bz2file-0.98 dm-sonnet-2.0.0 gevent-21.8.0 gunicorn-20.1.0 kfac-0.2.0 librosa-0.7.2 llvmlite-0.32.1 magenta-2.1.3 mesh-tensorflow-0.1.19 mido-1.2.6 mir-eval-0.6 note-seq-0.0.3 numba-0.49.1 pydub-0.25.1 pygtrie-2.4.2 pypng-0.0.20 python-rtmidi-1.1.2 sk-video-1.1.10 sox-1.4.1 tensor2tensor-1.15.7 tensorflow-addons-0.13.0 tensorflow-gan-2.0.0 tensorflow-probability-0.7.0 tf-slim-1.1.0 zope.event-4.5.0 zope.interface-5.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTU9nEcdj26b"
      },
      "source": [
        "**SUBFOLDERS**\n",
        "\n",
        "In oder to make this work you need to create 4 subdirectories:\n",
        "\n",
        "midi - for the initial midi files\n",
        "\n",
        "miditf - for the converted tensor flow input data\n",
        "\n",
        "midirun - for the model\n",
        "\n",
        "midiout - for the generated midi files\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx1ptrjdi3A4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b297bff5-f482-4fbd-b835-e29a3bd73097"
      },
      "source": [
        "#@title Drive Setup\n",
        "#@markdown If your training sample is in google drive you need to connect it. \n",
        "#@markdown You can also upload the data to a temporary folder but it will be \n",
        "#@markdown lost when the session is closed.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O976jh2chLLp",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe9a34d-8eb0-4b8f-8883-75afff9ccc86"
      },
      "source": [
        "#@title Import Dependencies\n",
        "#@markdown Import libraries from Magenta, Tensorflow and Numpy\n",
        "\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import magenta.music as mm\n",
        "import magenta\n",
        "from magenta.scripts import convert_dir_to_note_sequences\n",
        "from magenta.models.polyphony_rnn import *\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-JSJ5d8xkXJ"
      },
      "source": [
        "# Sample Adaptation\n",
        "\n",
        "Magenta does not work directly with Midi files but with NoteSequences, so the first step to create the sample is to convert the Midi files into Note sequences files and pack them as tfrecord to work with them fast and efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqVTh_Xs56vN"
      },
      "source": [
        "\n",
        "You need to define the folder with the midi files, as well as the\n",
        "folder in which the notesequence will be saved. Change the routes from below to your own folders. The output of this step (notesequences)  can be saved directly as a temporary file, e.g. '/tmp/notesequences.tfrecord'  instead of on drive, since after the next step in which the sample is split it would no longer be necessary.  \n",
        "\n",
        "Tip for novices: You can see the path of the folder to the left in files and with the mouse button select \"copy path\". As the route usually contains   \"My drive\" remember to enter \"\\\\\"  in the middel as \"My \\ drive\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ksd7ZJLG55Ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c530d2db-c17b-46d8-b211-12c6c6e6f223"
      },
      "source": [
        "!convert_dir_to_note_sequences \\\n",
        "--input_dir=/content/drive/MyDrive/midi/ \\\n",
        "--output_file=/temp/notesequences.tfrecord \\\n",
        "--log=INFO"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-08 11:21:13.750557: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "INFO:tensorflow:Converting files in '/content/drive/MyDrive/midi/'.\n",
            "I0808 11:21:16.433691 140079578769280 convert_dir_to_note_sequences.py:83] Converting files in '/content/drive/MyDrive/midi/'.\n",
            "INFO:tensorflow:0 files converted.\n",
            "I0808 11:21:16.434961 140079578769280 convert_dir_to_note_sequences.py:89] 0 files converted.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/07_torneo.mid.\n",
            "I0808 11:21:17.031405 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/07_torneo.mid.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/08_battala.mid.\n",
            "I0808 11:21:17.643438 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/08_battala.mid.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/05_espanoleta.mid.\n",
            "I0808 11:21:18.242431 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/05_espanoleta.mid.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/03_villano.mid.\n",
            "I0808 11:21:18.825139 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/03_villano.mid.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/02_mariona.mid.\n",
            "I0808 11:21:19.415224 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/02_mariona.mid.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/06_pavana.mid.\n",
            "I0808 11:21:19.894882 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/06_pavana.mid.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/04_las_hachas.mid.\n",
            "I0808 11:21:20.345185 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/04_las_hachas.mid.\n",
            "INFO:tensorflow:Converted MIDI file /content/drive/MyDrive/midi/01_gallarda.mid.\n",
            "I0808 11:21:20.798775 140079578769280 convert_dir_to_note_sequences.py:153] Converted MIDI file /content/drive/MyDrive/midi/01_gallarda.mid.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMfaieWF6tLe"
      },
      "source": [
        "Now you are ready to separate your sample between train and test. The percentage you leave in the test for evaluate your model is defined with the eval ratio argument. For example with a eval ratio equal to 10%, the 90% of the sample will be saved in the traing collection, while the remaining 10% will be stored as evaluation sample.\n",
        "\n",
        "The input for this step must match the one defined as output in the previous step.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIC9rvO2FfiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a449eb7-dfa0-4a0d-f63b-5b02c4c85e78"
      },
      "source": [
        "#test and train sample split wih 10% ratio\n",
        "!polyphony_rnn_create_dataset \\\n",
        "--input=/temp/notesequences.tfrecord \\\n",
        "--output_dir=/content/drive/MyDrive/miditf \\\n",
        "--eval_ratio=0.10 \\\n",
        "--config='polyphony'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-08 11:21:30.556121: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/pipelines/pipeline.py:310: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "W0808 11:21:34.791281 140197524084608 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/pipelines/pipeline.py:310: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:34.798074 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:35.554313 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:35.555053 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:35.904782 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:36.405207 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:36.415946 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:36.562524 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:37.091191 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "WARNING:tensorflow:Key signatures ignored by TranspositionPipeline.\n",
            "W0808 11:21:37.362590 140197524084608 note_sequence_pipelines.py:174] Key signatures ignored by TranspositionPipeline.\n",
            "INFO:tensorflow:\n",
            "\n",
            "Completed.\n",
            "\n",
            "I0808 11:21:37.715790 140197524084608 pipeline.py:386] \n",
            "\n",
            "Completed.\n",
            "\n",
            "INFO:tensorflow:Processed 8 inputs total. Produced 72 outputs.\n",
            "I0808 11:21:37.716024 140197524084608 pipeline.py:388] Processed 8 inputs total. Produced 72 outputs.\n",
            "INFO:tensorflow:DAGPipeline_PolyExtractor_training_polyphonic_track_lengths_in_bars:\n",
            "  [1,10): 27\n",
            "  [10,20): 27\n",
            "  [20,30): 18\n",
            "I0808 11:21:37.716140 140197524084608 statistics.py:137] DAGPipeline_PolyExtractor_training_polyphonic_track_lengths_in_bars:\n",
            "  [1,10): 27\n",
            "  [10,20): 27\n",
            "  [20,30): 18\n",
            "INFO:tensorflow:DAGPipeline_PolyExtractor_training_polyphonic_tracks_discarded_more_than_1_program: 0\n",
            "I0808 11:21:37.716244 140197524084608 statistics.py:137] DAGPipeline_PolyExtractor_training_polyphonic_tracks_discarded_more_than_1_program: 0\n",
            "INFO:tensorflow:DAGPipeline_PolyExtractor_training_polyphonic_tracks_discarded_too_long: 0\n",
            "I0808 11:21:37.716320 140197524084608 statistics.py:137] DAGPipeline_PolyExtractor_training_polyphonic_tracks_discarded_too_long: 0\n",
            "INFO:tensorflow:DAGPipeline_PolyExtractor_training_polyphonic_tracks_discarded_too_short: 9\n",
            "I0808 11:21:37.716396 140197524084608 statistics.py:137] DAGPipeline_PolyExtractor_training_polyphonic_tracks_discarded_too_short: 9\n",
            "INFO:tensorflow:DAGPipeline_RandomPartition_training_poly_tracks_count: 8\n",
            "I0808 11:21:37.716476 140197524084608 statistics.py:137] DAGPipeline_RandomPartition_training_poly_tracks_count: 8\n",
            "INFO:tensorflow:DAGPipeline_TranspositionPipeline_training_skipped_due_to_range_exceeded: 0\n",
            "I0808 11:21:37.716544 140197524084608 statistics.py:137] DAGPipeline_TranspositionPipeline_training_skipped_due_to_range_exceeded: 0\n",
            "INFO:tensorflow:DAGPipeline_TranspositionPipeline_training_transpositions_generated: 81\n",
            "I0808 11:21:37.716619 140197524084608 statistics.py:137] DAGPipeline_TranspositionPipeline_training_transpositions_generated: 81\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj5VWN5k87qM"
      },
      "source": [
        "If the cell has been executed correctly, you have to have two files saved in the output_dir, both in tfrecord format, one with the training sample and one with the eval sample. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0-HHl_H5bKE"
      },
      "source": [
        "# Model Training\n",
        "Now you are ready to train your model!\n",
        "\n",
        "This step can take a long, long time and depending on how large your database is and the number of layers and their size, you may get a memory error, or lose the connection to the GPU.\n",
        "\n",
        "We recommend you to start with a small sample and a light model for example a bach size of 64 and two LSTM rnn layers of 64, \"batch_size=64,rnn_layer_sizes=[64,64]\" and incorporate more complexity little by little. If you save the checkpoints you can re-launch the training at the point where you left the previous session, this is especially interesting if you lose the web connection or your session closes unexpectedly.  \n",
        "\n",
        "\n",
        "To train the model you can define the following parameters: \n",
        "* **run_dir** is the directory where checkpoints and TensorBoard data will be stored.\n",
        "* **sequence_example_file** is the TFRecord file with the train sample, the folder  must be the same as the one defined in output_dir in the previous step .\n",
        "* **num_training_steps** is an optional parameter for how many update steps to take before exiting the training loop. By default, training will run continuously until manually terminated.\n",
        "* **hparams** is another optional parameter that specifies the hyperparameters you want to use; batch size and RNN Layers in a vector with the units considered in each layer. \n",
        "* **dropout_keep_prob** is a optional parameter to reduce overfitting and improving model performance. Dropout is a regularization method to select randomly a % of neurons in the LSTM units thats are probabilistically excluded from activation and weight updates while training the model. \n",
        "* **learning_rate** is another optional parameter that controls how quickly or slowly a neural network model learns. This value is usually between 0.0 and 1.0, a learning rate too small may result in a long training process that could get stuck, whereas a value too large may result in an unstable training process.\n",
        "* **clip_norm** is another optional parameter. Gradient clipping clips parameters' gradients during backpropagation by a maximum norm to prevent Vanishing/Exploding gradients.\n",
        "\n",
        "By default polyphony_rnn model use this configuration: \n",
        "* batch_size=64,\n",
        "* rnn_layer_sizes=[256, 256, 256]\n",
        "* dropout_keep_prob=0.5\n",
        "* learning_rate=0.001\n",
        "* clip_norm=5\n",
        "\n",
        "Tip for novices: if you change the hyperparameters, for example by increasing the number of layers, remember to change the directory where the checkpoints are stored, otherwise the model will try to link to the last training and will give you an error of layer dimensions. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9LAnyFhPUOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "014a96fe-7c65-4da6-c85b-fb4ba64b7af1"
      },
      "source": [
        "#Train the model!\n",
        "!polyphony_rnn_train \\\n",
        "--run_dir=/content/drive/MyDrive/midirun/run1 \\\n",
        "--sequence_example_file=/content/drive/MyDrive/miditf/training_poly_tracks.tfrecord \\\n",
        "--num_training_steps=1000 \\\n",
        "--hparams=\"batch_size=4,rnn_layer_sizes=[128,128,128]\" \\\n",
        "--config='polyphony' \\\n",
        "--num_checkpoints=10\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-08 11:22:03.742542: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "INFO:tensorflow:hparams = {'batch_size': 4, 'rnn_layer_sizes': [128, 128, 128], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\n",
            "I0808 11:22:06.622708 140574635267968 events_rnn_graph.py:96] hparams = {'batch_size': 4, 'rnn_layer_sizes': [128, 128, 128], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\n",
            "INFO:tensorflow:Train dir: /content/drive/MyDrive/midirun/run1/train\n",
            "I0808 11:22:06.964729 140574635267968 polyphony_rnn_train.py:94] Train dir: /content/drive/MyDrive/midirun/run1/train\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0808 11:22:06.973248 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:66: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "W0808 11:22:06.986987 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:277: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "W0808 11:22:06.987617 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0808 11:22:06.988895 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:198: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0808 11:22:06.989629 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/input.py:198: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
            "W0808 11:22:06.992001 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:67: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n",
            "INFO:tensorflow:Counting records in /content/drive/MyDrive/miditf/training_poly_tracks.tfrecord.\n",
            "I0808 11:22:06.998392 140574635267968 sequence_example_lib.py:121] Counting records in /content/drive/MyDrive/miditf/training_poly_tracks.tfrecord.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "W0808 11:22:06.998523 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:122: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "INFO:tensorflow:Total records: 72\n",
            "I0808 11:22:07.096137 140574635267968 sequence_example_lib.py:127] Total records: 72\n",
            "INFO:tensorflow:[<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\n",
            "I0808 11:22:07.098964 140574635267968 sequence_example_lib.py:99] [<tf.Tensor 'random_shuffle_queue_Dequeue:0' shape=(?, 259) dtype=float32>, <tf.Tensor 'random_shuffle_queue_Dequeue:1' shape=(?,) dtype=int64>, <tf.Tensor 'random_shuffle_queue_Dequeue:2' shape=() dtype=int32>]\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "W0808 11:22:07.099242 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/common/sequence_example_lib.py:106: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:708: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0808 11:22:07.133640 140574635267968 rnn_cell_impl.py:1248] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0808 11:22:07.162066 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:759: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0808 11:22:07.223956 140574635267968 deprecation.py:534] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:759: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:173: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0808 11:22:07.353062 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:173: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0808 11:22:07.355653 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:186: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "INFO:tensorflow:Starting training loop...\n",
            "I0808 11:22:08.038244 140574635267968 events_rnn_train.py:103] Starting training loop...\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0808 11:22:08.038512 140574635267968 basic_session_run_hooks.py:546] Create CheckpointSaverHook.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0808 11:22:08.145931 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0808 11:22:08.219133 140574635267968 monitored_session.py:247] Graph was finalized.\n",
            "2021-08-08 11:22:08.225641: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-08 11:22:08.281662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:08.282300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:22:08.282348: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:22:08.321599: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-08 11:22:08.321762: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-08-08 11:22:08.421838: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-08-08 11:22:08.438368: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-08-08 11:22:08.625781: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-08-08 11:22:08.636593: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-08-08 11:22:08.636940: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-08 11:22:08.637101: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:08.637900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:08.638547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:22:08.638992: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-08-08 11:22:08.639274: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:08.639929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:22:08.640028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:08.640735: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:08.641368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:22:08.644487: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:22:11.175221: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-08 11:22:11.175278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-08 11:22:11.175301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-08 11:22:11.175537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:11.176318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:11.177004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:11.177638: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-08 11:22:11.177688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/midirun/run1/train/model.ckpt-69\n",
            "I0808 11:22:11.814569 140574635267968 saver.py:1298] Restoring parameters from /content/drive/MyDrive/midirun/run1/train/model.ckpt-69\n",
            "2021-08-08 11:22:11.850410: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000179999 Hz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1078: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "W0808 11:22:14.011286 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1078: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0808 11:22:14.049844 140574635267968 session_manager.py:531] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0808 11:22:14.055651 140574635267968 session_manager.py:534] Done running local_init_op.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py:907: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "W0808 11:22:14.074707 140574635267968 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py:907: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "To construct input pipelines, use the `tf.data` module.\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 69...\n",
            "I0808 11:22:14.950415 140574635267968 basic_session_run_hooks.py:614] Calling checkpoint listeners before saving checkpoint 69...\n",
            "INFO:tensorflow:Saving checkpoints for 69 into /content/drive/MyDrive/midirun/run1/train/model.ckpt.\n",
            "I0808 11:22:14.952046 140574635267968 basic_session_run_hooks.py:618] Saving checkpoints for 69 into /content/drive/MyDrive/midirun/run1/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 69...\n",
            "I0808 11:22:15.170315 140574635267968 basic_session_run_hooks.py:626] Calling checkpoint listeners after saving checkpoint 69...\n",
            "2021-08-08 11:22:15.559004: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:15.560057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:22:15.560279: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:15.561237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:15.562129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:22:15.562182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-08 11:22:15.562212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-08 11:22:15.562228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-08 11:22:15.562361: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:15.563426: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:22:15.564397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "2021-08-08 11:22:15.627345: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-08 11:22:17.246099: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "INFO:tensorflow:Accuracy = 0.3055326, Global Step = 69, Loss = 3.468714, Perplexity = 32.09544\n",
            "I0808 11:22:19.764776 140574635267968 basic_session_run_hooks.py:262] Accuracy = 0.3055326, Global Step = 69, Loss = 3.468714, Perplexity = 32.09544\n",
            "INFO:tensorflow:Accuracy = 0.30862328, Global Step = 79, Loss = 3.374221, Perplexity = 29.20153 (17.098 sec)\n",
            "I0808 11:22:36.863171 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.30862328, Global Step = 79, Loss = 3.374221, Perplexity = 29.20153 (17.098 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.584847\n",
            "I0808 11:22:36.864197 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.584847\n",
            "INFO:tensorflow:Accuracy = 0.29434547, Global Step = 89, Loss = 3.4212823, Perplexity = 30.608639 (15.150 sec)\n",
            "I0808 11:22:52.012893 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.29434547, Global Step = 89, Loss = 3.4212823, Perplexity = 30.608639 (15.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.660079\n",
            "I0808 11:22:52.013862 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.660079\n",
            "INFO:tensorflow:Accuracy = 0.33333334, Global Step = 99, Loss = 3.3858237, Perplexity = 29.542316 (16.262 sec)\n",
            "I0808 11:23:08.274679 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.33333334, Global Step = 99, Loss = 3.3858237, Perplexity = 29.542316 (16.262 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.614945\n",
            "I0808 11:23:08.275512 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.614945\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 105...\n",
            "I0808 11:23:16.862107 140574635267968 basic_session_run_hooks.py:614] Calling checkpoint listeners before saving checkpoint 105...\n",
            "INFO:tensorflow:Saving checkpoints for 105 into /content/drive/MyDrive/midirun/run1/train/model.ckpt.\n",
            "I0808 11:23:16.862361 140574635267968 basic_session_run_hooks.py:618] Saving checkpoints for 105 into /content/drive/MyDrive/midirun/run1/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 105...\n",
            "I0808 11:23:17.003919 140574635267968 basic_session_run_hooks.py:626] Calling checkpoint listeners after saving checkpoint 105...\n",
            "INFO:tensorflow:Accuracy = 0.28991905, Global Step = 109, Loss = 3.4312189, Perplexity = 30.914299 (18.318 sec)\n",
            "I0808 11:23:26.592670 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.28991905, Global Step = 109, Loss = 3.4312189, Perplexity = 30.914299 (18.318 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.545906\n",
            "I0808 11:23:26.593677 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.545906\n",
            "INFO:tensorflow:Accuracy = 0.27950898, Global Step = 119, Loss = 3.6712227, Perplexity = 39.299927 (16.532 sec)\n",
            "I0808 11:23:43.124471 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.27950898, Global Step = 119, Loss = 3.6712227, Perplexity = 39.299927 (16.532 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.604895\n",
            "I0808 11:23:43.125470 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.604895\n",
            "INFO:tensorflow:Accuracy = 0.2973761, Global Step = 129, Loss = 3.4114275, Perplexity = 30.308477 (16.896 sec)\n",
            "I0808 11:24:00.020135 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.2973761, Global Step = 129, Loss = 3.4114275, Perplexity = 30.308477 (16.896 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.591871\n",
            "I0808 11:24:00.021038 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.591871\n",
            "INFO:tensorflow:Accuracy = 0.2972973, Global Step = 139, Loss = 3.3275774, Perplexity = 27.870739 (15.054 sec)\n",
            "I0808 11:24:15.074089 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.2972973, Global Step = 139, Loss = 3.3275774, Perplexity = 27.870739 (15.054 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.664259\n",
            "I0808 11:24:15.075428 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.664259\n",
            "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 142...\n",
            "I0808 11:24:18.670019 140574635267968 basic_session_run_hooks.py:614] Calling checkpoint listeners before saving checkpoint 142...\n",
            "INFO:tensorflow:Saving checkpoints for 142 into /content/drive/MyDrive/midirun/run1/train/model.ckpt.\n",
            "I0808 11:24:18.670277 140574635267968 basic_session_run_hooks.py:618] Saving checkpoints for 142 into /content/drive/MyDrive/midirun/run1/train/model.ckpt.\n",
            "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 142...\n",
            "I0808 11:24:18.795721 140574635267968 basic_session_run_hooks.py:626] Calling checkpoint listeners after saving checkpoint 142...\n",
            "INFO:tensorflow:Accuracy = 0.3110119, Global Step = 149, Loss = 3.2849302, Perplexity = 26.70712 (17.410 sec)\n",
            "I0808 11:24:32.484046 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.3110119, Global Step = 149, Loss = 3.2849302, Perplexity = 26.70712 (17.410 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.574396\n",
            "I0808 11:24:32.485012 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.574396\n",
            "INFO:tensorflow:Accuracy = 0.26890758, Global Step = 159, Loss = 3.4360523, Perplexity = 31.064085 (16.886 sec)\n",
            "I0808 11:24:49.370456 140574635267968 basic_session_run_hooks.py:260] Accuracy = 0.26890758, Global Step = 159, Loss = 3.4360523, Perplexity = 31.064085 (16.886 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.592195\n",
            "I0808 11:24:49.371377 140574635267968 basic_session_run_hooks.py:702] global_step/sec: 0.592195\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/polyphony_rnn_train\", line 8, in <module>\n",
            "    sys.exit(console_entry_point())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 114, in console_entry_point\n",
            "    tf.app.run(main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/magenta/models/polyphony_rnn/polyphony_rnn_train.py\", line 110, in main\n",
            "    checkpoints_to_keep=FLAGS.num_checkpoints)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_train.py\", line 112, in run_training\n",
            "    is_chief=task == 0)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tf_slim/training/training.py\", line 551, in train\n",
            "    loss = session.run(train_op, run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 779, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1284, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1370, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1443, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/monitored_session.py\", line 1201, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 968, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1191, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1369, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1375, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1360, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\", line 1453, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMi2Cd4MleX"
      },
      "source": [
        "When you consider that the model is sufficiently tuned you can keep it in a bundle file. This allows you to import the trained model at any time and use it to create new sequences. To save it you have to call the same function of the previous step polyphony_rnn_generate, but changing some of the parameterization\n",
        "\n",
        "*   the run directory has to be the same as in previous step \n",
        "*   hparam must also be the same as those defined in the training. \n",
        "*   bundle_file is the path where to save the file with the model.mag\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilUtMeudzrHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "232e08ae-9ce4-4911-e679-29aef475ec02"
      },
      "source": [
        "#Save your model \n",
        "!polyphony_rnn_generate \\\n",
        "--run_dir=/content/drive/MyDrive/midirun/run1 \\\n",
        "--hparams=\"batch_size=64,rnn_layer_sizes=[128,128,128]\" \\\n",
        "--bundle_file=/content/drive/MyDrive/midirun/run1/my_poly_rnn.mag \\\n",
        "--config='polyphony' \\\n",
        "--save_generator_bundle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-08 11:25:10.018866: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:No bundle description provided.\n",
            "W0808 11:25:12.933130 140401442162560 polyphony_rnn_generate.py:264] No bundle description provided.\n",
            "INFO:tensorflow:Saving generator bundle to /content/drive/MyDrive/midirun/run1/my_poly_rnn.mag\n",
            "I0808 11:25:12.933340 140401442162560 polyphony_rnn_generate.py:265] Saving generator bundle to /content/drive/MyDrive/midirun/run1/my_poly_rnn.mag\n",
            "WARNING:tensorflow:Writing bundle file with no bundle description.\n",
            "W0808 11:25:12.933622 140401442162560 sequence_generator.py:219] Writing bundle file with no bundle description.\n",
            "INFO:tensorflow:hparams = {'batch_size': 1, 'rnn_layer_sizes': [128, 128, 128], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\n",
            "I0808 11:25:12.938129 140401442162560 events_rnn_graph.py:96] hparams = {'batch_size': 1, 'rnn_layer_sizes': [128, 128, 128], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:708: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0808 11:25:12.969294 140401442162560 rnn_cell_impl.py:1248] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0808 11:25:12.990319 140401442162560 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:759: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0808 11:25:13.044901 140401442162560 deprecation.py:534] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:759: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:253: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0808 11:25:13.131566 140401442162560 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:253: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "2021-08-08 11:25:13.142740: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-08 11:25:13.173929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.174685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:25:13.174734: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:25:13.181532: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-08 11:25:13.181647: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-08-08 11:25:13.192870: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-08-08 11:25:13.193273: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-08-08 11:25:13.200687: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-08-08 11:25:13.201429: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-08-08 11:25:13.201671: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-08 11:25:13.201806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.202601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.203270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:25:13.203686: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-08-08 11:25:13.203942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.204662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:25:13.204768: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.205502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.206134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:25:13.206209: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:25:13.736757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-08 11:25:13.736812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-08 11:25:13.736827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-08 11:25:13.737054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.737917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.738631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:13.739262: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-08 11:25:13.739311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "INFO:tensorflow:Checkpoint used: /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "I0808 11:25:13.739870 140401442162560 model.py:57] Checkpoint used: /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "I0808 11:25:13.740475 140401442162560 saver.py:1298] Restoring parameters from /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "2021-08-08 11:25:13.746078: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000179999 Hz\n",
            "WARNING:tensorflow:*******************************************************\n",
            "W0808 11:25:13.782996 140401442162560 saver.py:1150] *******************************************************\n",
            "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
            "W0808 11:25:13.783211 140401442162560 saver.py:1151] TensorFlow's V1 checkpoint format has been deprecated.\n",
            "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
            "W0808 11:25:13.783303 140401442162560 saver.py:1152] Consider switching to the more efficient V2 format:\n",
            "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
            "W0808 11:25:13.783377 140401442162560 saver.py:1153]    `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
            "WARNING:tensorflow:now on by default.\n",
            "W0808 11:25:13.783446 140401442162560 saver.py:1154] now on by default.\n",
            "WARNING:tensorflow:*******************************************************\n",
            "W0808 11:25:13.783526 140401442162560 saver.py:1155] *******************************************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6O27QcYPMQS"
      },
      "source": [
        "# Generating polyphonic tracks \n",
        "New tracks can be generated from the last saved checkpoint of the model (save in run_dir) or from the bundle, here is an example of both features. \n",
        "In addition the sequence can be started from the first notes of a midi file or directly by giving the notes. \n",
        "\n",
        "##Generation from a check point\n",
        "When you create a new melody from the last checkpoint trained you can do it at the end of the process or during the training to analyze the fit of the model. The training function also allows you to evaluate the model in the test sample, but what better test than the human ear? \n",
        "In fact this type of models are usually evaluated through listening test when  participants are asked to rate the generated sample in terms of the Likert scale, see for example the evaluation of [music transformed model](https://arxiv.org/pdf/1809.04281.pdf). \n",
        "As in the previous cases the run_dir must be the same path where the checkpoints have been saved, in output_dir you must put the path of the directory where you want to save the new creations. \n",
        "num_outputs gives the number of samples to generate and num_steps the length of the track. \n",
        "In this case, the generation is produced from three notes in Midi [language](https://newt.phys.unsw.edu.au/jw/notes.html) inserted as primer_pitches.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkcTsnITxCWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fb7be1-403c-434c-99c6-649b3fe9c46a"
      },
      "source": [
        "#generate new track with the trained model from a sequence of notes\n",
        "!polyphony_rnn_generate \\\n",
        "--config='polyphony' \\\n",
        "--run_dir=/content/drive/MyDrive/midirun/run1 \\\n",
        "--output_dir=/content/drive/MyDrive/midiout/poly_train1 \\\n",
        "--hparams=\"batch_size=64,rnn_layer_sizes=[128,128,128]\" \\\n",
        "--num_outputs=10 \\\n",
        "--num_steps=200 \\\n",
        "--primer_pitches=\"[67,64,60]\" \\\n",
        "--condition_on_primer=true \\\n",
        "--inject_primer_during_generation=false"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-08 11:25:24.421933: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "INFO:tensorflow:hparams = {'batch_size': 1, 'rnn_layer_sizes': [128, 128, 128], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\n",
            "I0808 11:25:27.334613 139893288535936 events_rnn_graph.py:96] hparams = {'batch_size': 1, 'rnn_layer_sizes': [128, 128, 128], 'dropout_keep_prob': 0.5, 'attn_length': 0, 'clip_norm': 5, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:708: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "W0808 11:25:27.366462 139893288535936 rnn_cell_impl.py:1248] `tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0808 11:25:27.386075 139893288535936 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:139: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:759: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0808 11:25:27.436665 139893288535936 deprecation.py:534] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:759: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:253: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "W0808 11:25:27.518847 139893288535936 deprecation.py:336] From /usr/local/lib/python3.7/dist-packages/magenta/models/shared/events_rnn_graph.py:253: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "2021-08-08 11:25:27.530804: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-08 11:25:27.556317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:27.557011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:25:27.557060: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:25:27.560083: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-08 11:25:27.560208: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-08-08 11:25:27.562218: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-08-08 11:25:27.562658: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-08-08 11:25:27.564692: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-08-08 11:25:27.565311: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-08-08 11:25:27.565572: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-08 11:25:27.565719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:27.566442: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:27.567032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:25:27.567404: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-08-08 11:25:27.567649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:27.568260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:25:27.568347: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:27.569493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:27.573056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:25:27.573144: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:25:28.096261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-08 11:25:28.096314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-08 11:25:28.096334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-08 11:25:28.096549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:28.097302: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:28.097946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:25:28.098540: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-08 11:25:28.098586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "INFO:tensorflow:Checkpoint used: /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "I0808 11:25:28.099061 139893288535936 model.py:57] Checkpoint used: /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "I0808 11:25:28.099633 139893288535936 saver.py:1298] Restoring parameters from /content/drive/MyDrive/midirun/run1/train/model.ckpt-142\n",
            "2021-08-08 11:25:28.104937: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000179999 Hz\n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:28.128383 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "2021-08-08 11:25:28.176687: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-08 11:25:28.582709: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3360.407227 \n",
            "I0808 11:25:31.339052 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3360.407227 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:31.625046 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3262.015869 \n",
            "I0808 11:25:34.393705 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3262.015869 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:34.406687 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3254.575195 \n",
            "I0808 11:25:37.079714 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3254.575195 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:37.091334 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3361.163574 \n",
            "I0808 11:25:39.670579 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3361.163574 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:39.683031 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3394.526367 \n",
            "I0808 11:25:42.505359 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3394.526367 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:42.517509 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3293.463623 \n",
            "I0808 11:25:45.265996 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3293.463623 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:45.277761 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3229.891357 \n",
            "I0808 11:25:48.056037 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3229.891357 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:48.067717 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3279.657471 \n",
            "I0808 11:25:50.847684 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3279.657471 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:50.860728 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3413.158936 \n",
            "I0808 11:25:53.660638 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3413.158936 \n",
            "INFO:tensorflow:Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "I0808 11:25:53.675147 139893288535936 polyphony_sequence_generator.py:173] Need to generate 195 more steps for this sequence, will try asking for 975 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -3374.328369 \n",
            "I0808 11:25:56.412495 139893288535936 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -3374.328369 \n",
            "INFO:tensorflow:Wrote 10 MIDI files to /content/drive/MyDrive/midiout/poly_train1\n",
            "I0808 11:25:56.425153 139893288535936 polyphony_rnn_generate.py:238] Wrote 10 MIDI files to /content/drive/MyDrive/midiout/poly_train1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2CSswPPxTNH"
      },
      "source": [
        "But to create a richer structure, it is better to start the sequence with a few seconds of a real track, this can be done by replacing primer_pitches by primer_midi, you have included the midi path you want to use. \n",
        "It is recommended to use only a few seconds, so before using it you can use the magenta library to extract some notes and save it in a temporal folder. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGlHFcsD4iJq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "46bf6e7f-28f7-4aa6-a69d-4cc8a4dbe2de"
      },
      "source": [
        "#Choose your own MIDI file\n",
        "primer_midi=(\"/content/drive/MyDrive/midiout/poly_train1/2021-08-08_110152_01.mid\")\n",
        "primer_ns=mm.midi_file_to_note_sequence(primer_midi)\n",
        "mm.plot_sequence(primer_ns)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "(function(root) {\n",
              "  function now() {\n",
              "    return new Date();\n",
              "  }\n",
              "\n",
              "  var force = true;\n",
              "\n",
              "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
              "    root._bokeh_onload_callbacks = [];\n",
              "    root._bokeh_is_loading = undefined;\n",
              "  }\n",
              "\n",
              "  var JS_MIME_TYPE = 'application/javascript';\n",
              "  var HTML_MIME_TYPE = 'text/html';\n",
              "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
              "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
              "\n",
              "  /**\n",
              "   * Render data to the DOM node\n",
              "   */\n",
              "  function render(props, node) {\n",
              "    var script = document.createElement(\"script\");\n",
              "    node.appendChild(script);\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when an output is cleared or removed\n",
              "   */\n",
              "  function handleClearOutput(event, handle) {\n",
              "    var cell = handle.cell;\n",
              "\n",
              "    var id = cell.output_area._bokeh_element_id;\n",
              "    var server_id = cell.output_area._bokeh_server_id;\n",
              "    // Clean up Bokeh references\n",
              "    if (id != null && id in Bokeh.index) {\n",
              "      Bokeh.index[id].model.document.clear();\n",
              "      delete Bokeh.index[id];\n",
              "    }\n",
              "\n",
              "    if (server_id !== undefined) {\n",
              "      // Clean up Bokeh references\n",
              "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
              "      cell.notebook.kernel.execute(cmd, {\n",
              "        iopub: {\n",
              "          output: function(msg) {\n",
              "            var id = msg.content.text.trim();\n",
              "            if (id in Bokeh.index) {\n",
              "              Bokeh.index[id].model.document.clear();\n",
              "              delete Bokeh.index[id];\n",
              "            }\n",
              "          }\n",
              "        }\n",
              "      });\n",
              "      // Destroy server and session\n",
              "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
              "      cell.notebook.kernel.execute(cmd);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  /**\n",
              "   * Handle when a new output is added\n",
              "   */\n",
              "  function handleAddOutput(event, handle) {\n",
              "    var output_area = handle.output_area;\n",
              "    var output = handle.output;\n",
              "\n",
              "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
              "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
              "      return\n",
              "    }\n",
              "\n",
              "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
              "\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
              "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
              "      // store reference to embed id on output_area\n",
              "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
              "    }\n",
              "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
              "      var bk_div = document.createElement(\"div\");\n",
              "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
              "      var script_attrs = bk_div.children[0].attributes;\n",
              "      for (var i = 0; i < script_attrs.length; i++) {\n",
              "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
              "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
              "      }\n",
              "      // store reference to server id on output_area\n",
              "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
              "    }\n",
              "  }\n",
              "\n",
              "  function register_renderer(events, OutputArea) {\n",
              "\n",
              "    function append_mime(data, metadata, element) {\n",
              "      // create a DOM node to render to\n",
              "      var toinsert = this.create_output_subarea(\n",
              "        metadata,\n",
              "        CLASS_NAME,\n",
              "        EXEC_MIME_TYPE\n",
              "      );\n",
              "      this.keyboard_manager.register_events(toinsert);\n",
              "      // Render to node\n",
              "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
              "      render(props, toinsert[toinsert.length - 1]);\n",
              "      element.append(toinsert);\n",
              "      return toinsert\n",
              "    }\n",
              "\n",
              "    /* Handle when an output is cleared or removed */\n",
              "    events.on('clear_output.CodeCell', handleClearOutput);\n",
              "    events.on('delete.Cell', handleClearOutput);\n",
              "\n",
              "    /* Handle when a new output is added */\n",
              "    events.on('output_added.OutputArea', handleAddOutput);\n",
              "\n",
              "    /**\n",
              "     * Register the mime type and append_mime function with output_area\n",
              "     */\n",
              "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
              "      /* Is output safe? */\n",
              "      safe: true,\n",
              "      /* Index of renderer in `output_area.display_order` */\n",
              "      index: 0\n",
              "    });\n",
              "  }\n",
              "\n",
              "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
              "  if (root.Jupyter !== undefined) {\n",
              "    var events = require('base/js/events');\n",
              "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
              "\n",
              "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
              "      register_renderer(events, OutputArea);\n",
              "    }\n",
              "  }\n",
              "\n",
              "  \n",
              "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
              "    root._bokeh_timeout = Date.now() + 5000;\n",
              "    root._bokeh_failed_load = false;\n",
              "  }\n",
              "\n",
              "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
              "     \"<div style='background-color: #fdd'>\\n\"+\n",
              "     \"<p>\\n\"+\n",
              "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
              "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
              "     \"</p>\\n\"+\n",
              "     \"<ul>\\n\"+\n",
              "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
              "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
              "     \"</ul>\\n\"+\n",
              "     \"<code>\\n\"+\n",
              "     \"from bokeh.resources import INLINE\\n\"+\n",
              "     \"output_notebook(resources=INLINE)\\n\"+\n",
              "     \"</code>\\n\"+\n",
              "     \"</div>\"}};\n",
              "\n",
              "  function display_loaded() {\n",
              "    var el = document.getElementById(null);\n",
              "    if (el != null) {\n",
              "      el.textContent = \"BokehJS is loading...\";\n",
              "    }\n",
              "    if (root.Bokeh !== undefined) {\n",
              "      if (el != null) {\n",
              "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
              "      }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(display_loaded, 100)\n",
              "    }\n",
              "  }\n",
              "\n",
              "\n",
              "  function run_callbacks() {\n",
              "    try {\n",
              "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
              "        if (callback != null)\n",
              "          callback();\n",
              "      });\n",
              "    } finally {\n",
              "      delete root._bokeh_onload_callbacks\n",
              "    }\n",
              "    console.debug(\"Bokeh: all callbacks have finished\");\n",
              "  }\n",
              "\n",
              "  function load_libs(css_urls, js_urls, callback) {\n",
              "    if (css_urls == null) css_urls = [];\n",
              "    if (js_urls == null) js_urls = [];\n",
              "\n",
              "    root._bokeh_onload_callbacks.push(callback);\n",
              "    if (root._bokeh_is_loading > 0) {\n",
              "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
              "      return null;\n",
              "    }\n",
              "    if (js_urls == null || js_urls.length === 0) {\n",
              "      run_callbacks();\n",
              "      return null;\n",
              "    }\n",
              "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
              "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
              "\n",
              "    function on_load() {\n",
              "      root._bokeh_is_loading--;\n",
              "      if (root._bokeh_is_loading === 0) {\n",
              "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
              "        run_callbacks()\n",
              "      }\n",
              "    }\n",
              "\n",
              "    function on_error(url) {\n",
              "      console.error(\"failed to load \" + url);\n",
              "    }\n",
              "\n",
              "    for (let i = 0; i < css_urls.length; i++) {\n",
              "      const url = css_urls[i];\n",
              "      const element = document.createElement(\"link\");\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.rel = \"stylesheet\";\n",
              "      element.type = \"text/css\";\n",
              "      element.href = url;\n",
              "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
              "      document.body.appendChild(element);\n",
              "    }\n",
              "\n",
              "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\": \"dM3QQsP+wXdHg42wTqW85BjZQdLNNIXqlPw/BgKoExPmTG7ZLML4EGqLMfqHT6ON\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\": \"8x57I4YuIfu8XyZfFo0XVr2WAT8EK4rh/uDe3wF7YuW2FNUSNEpJbsPaB1nJ2fz2\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\": \"3QTqdz9LyAm2i0sG5XTePsHec3UHWwVsrOL68SYRoAXsafvfAyqtQ+h440+qIBhS\"};\n",
              "\n",
              "    for (let i = 0; i < js_urls.length; i++) {\n",
              "      const url = js_urls[i];\n",
              "      const element = document.createElement('script');\n",
              "      element.onload = on_load;\n",
              "      element.onerror = on_error.bind(null, url);\n",
              "      element.async = false;\n",
              "      element.src = url;\n",
              "      if (url in hashes) {\n",
              "        element.crossOrigin = \"anonymous\";\n",
              "        element.integrity = \"sha384-\" + hashes[url];\n",
              "      }\n",
              "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
              "      document.head.appendChild(element);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  function inject_raw_css(css) {\n",
              "    const element = document.createElement(\"style\");\n",
              "    element.appendChild(document.createTextNode(css));\n",
              "    document.body.appendChild(element);\n",
              "  }\n",
              "\n",
              "  \n",
              "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\"];\n",
              "  var css_urls = [];\n",
              "  \n",
              "\n",
              "  var inline_js = [\n",
              "    function(Bokeh) {\n",
              "      Bokeh.set_log_level(\"info\");\n",
              "    },\n",
              "    function(Bokeh) {\n",
              "    \n",
              "    \n",
              "    }\n",
              "  ];\n",
              "\n",
              "  function run_inline_js() {\n",
              "    \n",
              "    if (root.Bokeh !== undefined || force === true) {\n",
              "      \n",
              "    for (var i = 0; i < inline_js.length; i++) {\n",
              "      inline_js[i].call(root, root.Bokeh);\n",
              "    }\n",
              "    } else if (Date.now() < root._bokeh_timeout) {\n",
              "      setTimeout(run_inline_js, 100);\n",
              "    } else if (!root._bokeh_failed_load) {\n",
              "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
              "      root._bokeh_failed_load = true;\n",
              "    } else if (force !== true) {\n",
              "      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n",
              "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
              "    }\n",
              "\n",
              "  }\n",
              "\n",
              "  if (root._bokeh_is_loading === 0) {\n",
              "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
              "    run_inline_js();\n",
              "  } else {\n",
              "    load_libs(css_urls, js_urls, function() {\n",
              "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
              "      run_inline_js();\n",
              "    });\n",
              "  }\n",
              "}(window));"
            ],
            "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\": \"dM3QQsP+wXdHg42wTqW85BjZQdLNNIXqlPw/BgKoExPmTG7ZLML4EGqLMfqHT6ON\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\": \"8x57I4YuIfu8XyZfFo0XVr2WAT8EK4rh/uDe3wF7YuW2FNUSNEpJbsPaB1nJ2fz2\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\": \"3QTqdz9LyAm2i0sG5XTePsHec3UHWwVsrOL68SYRoAXsafvfAyqtQ+h440+qIBhS\"};\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.3.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.3.3.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "\n",
              "  <div class=\"bk-root\" id=\"6693de5a-557a-4f9d-be89-2121d4798fef\" data-root-id=\"1002\"></div>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "(function(root) {\n",
              "  function embed_document(root) {\n",
              "    \n",
              "  var docs_json = {\"37f72406-faf0-4f62-88dd-f61551a3f3d9\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1011\"}],\"center\":[{\"id\":\"1014\"},{\"id\":\"1018\"}],\"height\":200,\"left\":[{\"id\":\"1015\"}],\"renderers\":[{\"id\":\"1038\"}],\"title\":{\"id\":\"1040\"},\"toolbar\":{\"id\":\"1025\"},\"width\":500,\"x_range\":{\"id\":\"1003\"},\"x_scale\":{\"id\":\"1007\"},\"y_range\":{\"id\":\"1005\"},\"y_scale\":{\"id\":\"1009\"}},\"id\":\"1002\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1007\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1020\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis\":{\"id\":\"1011\"},\"ticker\":null},\"id\":\"1014\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1043\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"bottom\":{\"field\":\"bottom\"},\"fill_alpha\":{\"field\":\"fill_alpha\"},\"fill_color\":{\"value\":\"#d53e4f\"},\"left\":{\"field\":\"start_time\"},\"right\":{\"field\":\"end_time\"},\"top\":{\"field\":\"top\"}},\"id\":\"1036\",\"type\":\"Quad\"},{\"attributes\":{\"active_multi\":null,\"tools\":[{\"id\":\"1019\"},{\"id\":\"1020\"},{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"}]},\"id\":\"1025\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1042\",\"type\":\"AllLabels\"},{\"attributes\":{\"source\":{\"id\":\"1034\"}},\"id\":\"1039\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"interval\":12},\"id\":\"1031\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"pitch\",\"@pitch\"],[\"program\",\"@program\"],[\"velo\",\"@velocity\"],[\"duration\",\"@duration\"],[\"start_time\",\"@start_time\"],[\"end_time\",\"@end_time\"],[\"velocity\",\"@velocity\"],[\"fill_alpha\",\"@fill_alpha\"]]},\"id\":\"1019\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1040\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1024\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"axis_label\":\"time (sec)\",\"formatter\":{\"id\":\"1046\"},\"major_label_policy\":{\"id\":\"1045\"},\"ticker\":{\"id\":\"1012\"}},\"id\":\"1011\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis\":{\"id\":\"1015\"},\"dimension\":1,\"ticker\":{\"id\":\"1033\"}},\"id\":\"1018\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"ResetTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1024\"}},\"id\":\"1021\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1003\",\"type\":\"DataRange1d\"},{\"attributes\":{\"bottom\":{\"field\":\"bottom\"},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#d53e4f\"},\"left\":{\"field\":\"start_time\"},\"line_alpha\":{\"value\":0.1},\"right\":{\"field\":\"end_time\"},\"top\":{\"field\":\"top\"}},\"id\":\"1037\",\"type\":\"Quad\"},{\"attributes\":{},\"id\":\"1045\",\"type\":\"AllLabels\"},{\"attributes\":{\"axis_label\":\"pitch (MIDI)\",\"formatter\":{\"id\":\"1043\"},\"major_label_policy\":{\"id\":\"1042\"},\"ticker\":{\"id\":\"1031\"}},\"id\":\"1015\",\"type\":\"LinearAxis\"},{\"attributes\":{\"interval\":12},\"id\":\"1033\",\"type\":\"SingleIntervalTicker\"},{\"attributes\":{\"data\":{\"bottom\":{\"__ndarray__\":\"zczMzMzMTUDNzMzMzMxPQGZmZmZmplBAZmZmZmYmUkBmZmZmZmYaQGZmZmZmJlBAZmZmZmZmUEDNzMzMzMxHQM3MzMzMzExAZmZmZmYmUUBmZmZmZmZQQGZmZmZmplFAzczMzMxMSUBmZmZmZiZRQGZmZmZm5lBAZmZmZmamUEDNzMzMzMxJQGZmZmZmJlBAzczMzMxMTUDNzMzMzExEQM3MzMzMzE1AZmZmZmZmUEBmZmZmZiZRQM3MzMzMzE1AZmZmZmbmUEBmZmZmZuZQQGZmZmZmZlRAmpmZmZmZMEDNzMzMzExLQM3MzMzMTExAZmZmZmamUEDNzMzMzExPQGZmZmZmZlFAZmZmZmZmUUBmZmZmZiZSQM3MzMzMTE9AZmZmZmZmUEDNzMzMzExOQM3MzMzMTEZAzczMzMzMSEBmZmZmZqZWQGZmZmZmplZAzczMzMxMTkDNzMzMzMxPQM3MzMzMzEZAZmZmZmZmUEBmZmZmZqZQQGZmZmZmplFAzczMzMzMTUDNzMzMzExOQM3MzMzMzE9AZmZmZmYmUEBmZmZmZiZWQM3MzMzMzE9AZmZmZmamUUDNzMzMzMxIQM3MzMzMzE1AZmZmZmbmUEDNzMzMzMxKQGZmZmZmJlBAzczMzMxMREBmZmZmZmZRQM3MzMzMzElAzczMzMzMTkBmZmZmZiZQQGZmZmZmJlFAzczMzMxMS0BmZmZmZiZQQM3MzMzMTEdAzczMzMxMSUDNzMzMzExHQM3MzMzMzEhAZmZmZmYmUEBmZmZmZiZQQM3MzMzMTEpAzczMzMxMS0DNzMzMzMxNQGZmZmZmJlFAzczMzMxMR0BmZmZmZmZQQGZmZmZm5l1AzczMzMxMTEBmZmZmZiZRQGZmZmZmZlFAzczMzMzMSEDNzMzMzExLQGZmZmZm5lBAZmZmZmZmVUDNzMzMzExMQGZmZmZm5lFAzczMzMxMTEBmZmZmZuZQQGZmZmZm5lBAZmZmZmZmUUDNzMzMzMxOQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[95]},\"duration\":{\"__ndarray__\":\"AAAAAAAA4D8AAAAAAADgPwAAAAAAAOA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADQPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADQPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAA0D8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAANA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAANA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPwAAAAAAAMA/AAAAAAAAwD8AAAAAAADAPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[95]},\"end_time\":{\"__ndarray__\":\"AAAAAAAA4D8AAAAAAADgPwAAAAAAAOA/AAAAAAAA6D8AAAAAAAD2PwAAAAAAAPo/AAAAAAAA/j8AAAAAAAAHQAAAAAAAAAhAAAAAAAAACUAAAAAAAAAPQAAAAAAAAA9AAAAAAACAEUAAAAAAAAASQAAAAAAAABZAAAAAAAAAGEAAAAAAAAAZQAAAAAAAABlAAAAAAACAGUAAAAAAAIAaQAAAAAAAgBtAAAAAAACAG0AAAAAAAIAbQAAAAAAAABxAAAAAAAAAHEAAAAAAAAAcQAAAAAAAgB1AAAAAAAAAHkAAAAAAAIAfQAAAAAAAQCBAAAAAAABAIEAAAAAAAIAhQAAAAAAAgCFAAAAAAACAIUAAAAAAAMAhQAAAAAAAACJAAAAAAABAIkAAAAAAAIAjQAAAAAAAgCRAAAAAAADAJEAAAAAAAMAkQAAAAAAAwCRAAAAAAABAJkAAAAAAAMAnQAAAAAAAwClAAAAAAADAKUAAAAAAAMApQAAAAAAAwClAAAAAAABAKkAAAAAAAIAqQAAAAAAAwCpAAAAAAABAK0AAAAAAAEArQAAAAAAAACxAAAAAAACALEAAAAAAAIAtQAAAAAAAgC5AAAAAAAAAL0AAAAAAAMAvQAAAAAAAQDBAAAAAAACAMEAAAAAAAOAwQAAAAAAAIDFAAAAAAAAgMUAAAAAAACAxQAAAAAAAIDJAAAAAAADgMkAAAAAAAOAyQAAAAAAAgDNAAAAAAADAM0AAAAAAAOAzQAAAAAAA4DNAAAAAAAAANEAAAAAAAAA0QAAAAAAAoDRAAAAAAADANEAAAAAAAMA0QAAAAAAA4DRAAAAAAACgNUAAAAAAAKA1QAAAAAAAoDVAAAAAAADANUAAAAAAAMA1QAAAAAAAwDVAAAAAAADgNUAAAAAAACA2QAAAAAAAQDZAAAAAAABANkAAAAAAAIA4QAAAAAAAgDhAAAAAAACgOEAAAAAAAKA4QAAAAAAAoDhAAAAAAACgOEAAAAAAAMA4QA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[95]},\"fill_alpha\":{\"__ndarray__\":\"AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPwAAAAAAAPA/AAAAAAAA8D8AAAAAAADwPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[95]},\"index\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94],\"instrument\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"pitch\":[60,64,67,73,7,65,66,48,58,69,66,71,51,69,68,67,52,65,59,41,60,66,69,60,68,68,82,17,55,57,67,63,70,70,73,63,66,61,45,50,91,91,61,64,46,66,67,71,60,61,64,65,89,64,71,50,60,68,54,65,41,70,52,62,65,69,55,65,47,51,47,50,65,65,53,55,60,69,47,66,120,57,69,70,50,55,68,86,57,72,57,68,68,70,62],\"program\":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\"start_time\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA5D8AAAAAAAD0PwAAAAAAAPg/AAAAAAAA/D8AAAAAAAAGQAAAAAAAAAdAAAAAAAAACEAAAAAAAAANQAAAAAAAAA5AAAAAAAAAEUAAAAAAAAARQAAAAAAAgBVAAAAAAACAF0AAAAAAAIAYQAAAAAAAgBhAAAAAAAAAGUAAAAAAAAAaQAAAAAAAABtAAAAAAAAAG0AAAAAAAAAbQAAAAAAAgBtAAAAAAACAG0AAAAAAAIAbQAAAAAAAAB1AAAAAAACAHUAAAAAAAAAfQAAAAAAAACBAAAAAAAAAIEAAAAAAAEAhQAAAAAAAQCFAAAAAAABAIUAAAAAAAIAhQAAAAAAAwCFAAAAAAAAAIkAAAAAAAEAjQAAAAAAAQCRAAAAAAACAJEAAAAAAAIAkQAAAAAAAgCRAAAAAAAAAJkAAAAAAAIAnQAAAAAAAgClAAAAAAABAKUAAAAAAAIApQAAAAAAAgClAAAAAAAAAKkAAAAAAAEAqQAAAAAAAgCpAAAAAAAAAK0AAAAAAAAArQAAAAAAAwCtAAAAAAABALEAAAAAAAEAtQAAAAAAAAC5AAAAAAADALkAAAAAAAIAvQAAAAAAAIDBAAAAAAABgMEAAAAAAAMAwQAAAAAAAADFAAAAAAAAAMUAAAAAAAAAxQAAAAAAAADJAAAAAAADAMkAAAAAAAMAyQAAAAAAAYDNAAAAAAACgM0AAAAAAAMAzQAAAAAAAwDNAAAAAAADgM0AAAAAAAOAzQAAAAAAAgDRAAAAAAACgNEAAAAAAAKA0QAAAAAAAwDRAAAAAAACANUAAAAAAAIA1QAAAAAAAgDVAAAAAAACgNUAAAAAAAKA1QAAAAAAAgDVAAAAAAADANUAAAAAAAAA2QAAAAAAAIDZAAAAAAAAgNkAAAAAAAGA4QAAAAAAAYDhAAAAAAACAOEAAAAAAAIA4QAAAAAAAgDhAAAAAAACAOEAAAAAAAKA4QA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[95]},\"top\":{\"__ndarray__\":\"MzMzMzMzTkCamZmZmRlQQJqZmZmZ2VBAmpmZmZlZUkCamZmZmZkdQJqZmZmZWVBAmpmZmZmZUEAzMzMzMzNIQDMzMzMzM01AmpmZmZlZUUCamZmZmZlQQJqZmZmZ2VFAMzMzMzOzSUCamZmZmVlRQJqZmZmZGVFAmpmZmZnZUEAzMzMzMzNKQJqZmZmZWVBAMzMzMzOzTUAzMzMzM7NEQDMzMzMzM05AmpmZmZmZUECamZmZmVlRQDMzMzMzM05AmpmZmZkZUUCamZmZmRlRQJqZmZmZmVRAZmZmZmZmMUAzMzMzM7NLQDMzMzMzs0xAmpmZmZnZUEAzMzMzM7NPQJqZmZmZmVFAmpmZmZmZUUCamZmZmVlSQDMzMzMzs09AmpmZmZmZUEAzMzMzM7NOQDMzMzMzs0ZAMzMzMzMzSUCamZmZmdlWQJqZmZmZ2VZAMzMzMzOzTkCamZmZmRlQQDMzMzMzM0dAmpmZmZmZUECamZmZmdlQQJqZmZmZ2VFAMzMzMzMzTkAzMzMzM7NOQJqZmZmZGVBAmpmZmZlZUECamZmZmVlWQJqZmZmZGVBAmpmZmZnZUUAzMzMzMzNJQDMzMzMzM05AmpmZmZkZUUAzMzMzMzNLQJqZmZmZWVBAMzMzMzOzRECamZmZmZlRQDMzMzMzM0pAMzMzMzMzT0CamZmZmVlQQJqZmZmZWVFAMzMzMzOzS0CamZmZmVlQQDMzMzMzs0dAMzMzMzOzSUAzMzMzM7NHQDMzMzMzM0lAmpmZmZlZUECamZmZmVlQQDMzMzMzs0pAMzMzMzOzS0AzMzMzMzNOQJqZmZmZWVFAMzMzMzOzR0CamZmZmZlQQJqZmZmZGV5AMzMzMzOzTECamZmZmVlRQJqZmZmZmVFAMzMzMzMzSUAzMzMzM7NLQJqZmZmZGVFAmpmZmZmZVUAzMzMzM7NMQJqZmZmZGVJAMzMzMzOzTECamZmZmRlRQJqZmZmZGVFAmpmZmZmZUUAzMzMzMzNPQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[95]},\"velocity\":[100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100,100]},\"selected\":{\"id\":\"1047\"},\"selection_policy\":{\"id\":\"1048\"}},\"id\":\"1034\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"1034\"},\"glyph\":{\"id\":\"1036\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1037\"},\"view\":{\"id\":\"1039\"}},\"id\":\"1038\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"SaveTool\"}],\"root_ids\":[\"1002\"]},\"title\":\"Bokeh Application\",\"version\":\"2.3.3\"}};\n",
              "  var render_items = [{\"docid\":\"37f72406-faf0-4f62-88dd-f61551a3f3d9\",\"root_ids\":[\"1002\"],\"roots\":{\"1002\":\"6693de5a-557a-4f9d-be89-2121d4798fef\"}}];\n",
              "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
              "\n",
              "  }\n",
              "  if (root.Bokeh !== undefined) {\n",
              "    embed_document(root);\n",
              "  } else {\n",
              "    var attempts = 0;\n",
              "    var timer = setInterval(function(root) {\n",
              "      if (root.Bokeh !== undefined) {\n",
              "        clearInterval(timer);\n",
              "        embed_document(root);\n",
              "      } else {\n",
              "        attempts++;\n",
              "        if (attempts > 100) {\n",
              "          clearInterval(timer);\n",
              "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
              "        }\n",
              "      }\n",
              "    }, 10, root)\n",
              "  }\n",
              "})(window);"
            ],
            "application/vnd.bokehjs_exec.v0+json": ""
          },
          "metadata": {
            "tags": [],
            "application/vnd.bokehjs_exec.v0+json": {
              "id": "1002"
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNOlRkY23UZf"
      },
      "source": [
        "#Generation from a Bundle\n",
        "Creating songs from a saved bundle is just as easy as changing run_dir for bundle_file, with the folder path where the bundle is stored.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zxy04r4U3c3o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db4361f-7608-449b-ffc0-b134ec9b147b"
      },
      "source": [
        "#generate new sequences with the train model (budle_file)\n",
        "!polyphony_rnn_generate \\\n",
        "--config='polyphony_rnn ' \\\n",
        "--bundle_file=/content/drive/MyDrive/midirun/run1/dowland_rnn_128.mag \\\n",
        "--output_dir=/content/drive/MyDrive/midiout/poly_train1 \\\n",
        "--num_outputs=3 \\\n",
        "--num_steps=100 \\\n",
        "--condition_on_primer=False \\\n",
        "--inject_primer_during_generation=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-08 11:34:25.432405: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.\n",
            "Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.\n",
            "  from numba.decorators import jit as optional_jit\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:No priming sequence specified. Defaulting to empty sequence.\n",
            "W0808 11:34:28.165541 139977906763648 polyphony_rnn_generate.py:186] No priming sequence specified. Defaulting to empty sequence.\n",
            "2021-08-08 11:34:28.169433: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-08-08 11:34:28.194903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.195843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:34:28.195905: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:34:28.199361: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-08 11:34:28.199458: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-08-08 11:34:28.202257: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-08-08 11:34:28.202605: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-08-08 11:34:28.204315: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-08-08 11:34:28.204867: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-08-08 11:34:28.205060: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-08-08 11:34:28.205192: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.205813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.206384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:34:28.206776: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-08-08 11:34:28.207011: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.207693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla V100-SXM2-16GB computeCapability: 7.0\n",
            "coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
            "2021-08-08 11:34:28.207786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.208398: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.208968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-08-08 11:34:28.209018: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-08-08 11:34:28.711507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-08-08 11:34:28.711558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-08-08 11:34:28.711572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-08-08 11:34:28.711784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.712465: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.713064: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-08-08 11:34:28.713642: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-08-08 11:34:28.713688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14682 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0)\n",
            "INFO:tensorflow:Restoring parameters from /tmp/tmpfranmsad/model.ckpt\n",
            "I0808 11:34:28.761763 139977906763648 saver.py:1298] Restoring parameters from /tmp/tmpfranmsad/model.ckpt\n",
            "2021-08-08 11:34:28.767062: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000179999 Hz\n",
            "INFO:tensorflow:Need to generate 99 more steps for this sequence, will try asking for 495 RNN steps\n",
            "I0808 11:34:28.785638 139977906763648 polyphony_sequence_generator.py:173] Need to generate 99 more steps for this sequence, will try asking for 495 RNN steps\n",
            "2021-08-08 11:34:28.827279: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-08-08 11:34:29.217939: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -151.909210 \n",
            "I0808 11:34:30.545294 139977906763648 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -151.909210 \n",
            "INFO:tensorflow:Need to generate 99 more steps for this sequence, will try asking for 495 RNN steps\n",
            "I0808 11:34:30.555367 139977906763648 polyphony_sequence_generator.py:173] Need to generate 99 more steps for this sequence, will try asking for 495 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -127.653305 \n",
            "I0808 11:34:31.876328 139977906763648 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -127.653305 \n",
            "INFO:tensorflow:Need to generate 99 more steps for this sequence, will try asking for 495 RNN steps\n",
            "I0808 11:34:31.885052 139977906763648 polyphony_sequence_generator.py:173] Need to generate 99 more steps for this sequence, will try asking for 495 RNN steps\n",
            "INFO:tensorflow:Beam search yields sequence with log-likelihood: -127.884399 \n",
            "I0808 11:34:33.219920 139977906763648 events_rnn_model.py:381] Beam search yields sequence with log-likelihood: -127.884399 \n",
            "INFO:tensorflow:Wrote 3 MIDI files to /content/drive/MyDrive/midiout/poly_train1\n",
            "I0808 11:34:33.229949 139977906763648 polyphony_rnn_generate.py:238] Wrote 3 MIDI files to /content/drive/MyDrive/midiout/poly_train1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}